{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pathlib\n",
    "import importlib\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import feather\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple\n",
    "from scipy import stats\n",
    "import dexplot as dxp\n",
    "import scipy.stats as ss\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import f1_score, roc_curve, auc, roc_auc_score\n",
    "from scikitplot.metrics import plot_confusion_matrix\n",
    "from scikitplot.metrics import plot_roc\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import tree\n",
    "from category_encoders.target_encoder import TargetEncoder\n"
   ]
  },
  {
   "source": [
    "# Outline\n",
    "* Preprocessing\n",
    "* Feature Engineering\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = pathlib.Path('./data')\n",
    "RESULTS_PATH = pathlib.Path('./results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH / 'accepted_2007_to_2018Q4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_list = ['addr_state', 'annual_inc', 'application_type', \n",
    "             'dti', 'earliest_cr_line', 'emp_length', 'fico_range_high', 'fico_range_low', \n",
    "             'grade', 'home_ownership', 'initial_list_status', \n",
    "             'installment', 'int_rate', 'issue_d', 'loan_amnt', \n",
    "             'mort_acc', 'open_acc', 'pub_rec', 'pub_rec_bankruptcies', \n",
    "             'purpose', 'revol_bal', 'revol_util', 'sub_grade', 'term', \n",
    "             'title', 'total_acc', 'verification_status', 'loan_status']\n",
    "keep_list.sort()\n",
    "df = df[keep_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['loan_status'].isin(['Fully Paid', 'Charged Off'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'] = 1\n",
    "df['target'] = (df['target'].where(df['loan_status'] == 'Charged Off', 0))\n",
    "df = df.drop('loan_status', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    0.800374\n",
       "1    0.199626\n",
       "Name: target, dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# Reminding ourselves of the data imbalance ratio\n",
    "df['target'].value_counts()/df['target'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = list(df.select_dtypes(include=['float64', 'int64']).columns)\n",
    "num_cols.remove('target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_cols = ['earliest_cr_line', 'issue_d']\n",
    "for c in dates_cols:\n",
    "    df[c] = pd.to_datetime(df[c], format='%b-%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = list(df.select_dtypes('object').columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with some preprocessing now that we have some understanding of the data.\n",
    "\n",
    "We need to start filling the missing values.\n",
    "\n",
    "* For numerical columns, for simplicity, we are going to fill missing values with the median.\n",
    "* For categorical columns, a simple approach would be to fill the missing values with the most common observation.\n",
    "\n",
    "Keep in mind that these approaches are just to keep things simple. If one thinks that more time should be spent on preprocessing, they could go with fancier methods, including:\n",
    "- using other ML models to come up with values for the missing \n",
    "\n",
    "When filling the missing values, it is important that we calculate this value using the training set only during the training phase, so as to prevent test set information from leaking. This kind of leakage can cause overfitting to the test set, artificially boosting the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [2013, 2014, 2015, 2016]\n",
    "train_years = years[0:3]\n",
    "test_years = years[3:]\n",
    "train_df = df[df['issue_d'].dt.year.isin(train_years)]\n",
    "test_df = df[df['issue_d'].dt.year.isin(test_years)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "emp_length              5.835904\n",
       "mort_acc                3.514506\n",
       "title                   1.238302\n",
       "revol_util              0.063703\n",
       "pub_rec_bankruptcies    0.051810\n",
       "dti                     0.027800\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "missing = (100*df.isnull().sum()/df.shape[0]).sort_values(ascending=False)\n",
    "missing[missing > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple approach for numerical features: Fill with the median\n",
    "\n",
    "For categorical features we can fill with the most frequent values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df, num_cols, cat_cols):\n",
    "\n",
    "    for col in num_cols:\n",
    "        median = df[col].median()\n",
    "        df[col] = df[col].fillna(median)\n",
    "\n",
    "    for col in cat_cols:\n",
    "        most_common = df[col].mode()[0]\n",
    "        df[col] = df[col].fillna(most_common)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Remark: the training and test medians might not be exactly the same, so the values filled would be different\n",
    "# This also applies to the most common observation, which may be different in the test set"
   ]
  },
  {
   "source": [
    "# Feature Engineering"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Now we do some basic feature engineering. \n",
    "For this workshop, this will consistly mostly of:\n",
    "* scaling the numerical features (required by some models, but not all of them)\n",
    "* encoding categorical features\n",
    "* regular expression with some specific features\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Scaling\n",
    "Some models rely on the distance between our datapoints in the feature space. Since distances can be scaled, these models can produce different results if features are not properly scaled. For example, if one numerical feature is always of the order of thousands, and another is of the order of units, the larger feature will tend to dominate any distance calculation.\n",
    "\n",
    "To solve that, we scale features to ensure that they are of the same order. There are various ways to do that.\n",
    "\n",
    "Here are a few:\n",
    "\n",
    "## Standard Scaler:\n",
    "Scales using the mean and standard deviation\n",
    "\n",
    "## Robust Scaler:\n",
    "Scales using the median and a certain quantile interval\n",
    "\n",
    "## Min-Max Scaler:\n",
    "Scales by ensuring that the minimum and maximum values are -1 and 1, respectively"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Encoding \n",
    "Before we move on, let's briefly discuss encoding.\n",
    "\n",
    "Most ML models require numeric inputs, which makes categorical features not usable in their raw form.\n",
    "Encoding is a process in which we map the categories into numbers based on some criterion.\n",
    "\n",
    "Here are a few examples of encodings:\n",
    "\n",
    " \n",
    "## Label Encoding:\n",
    " * First Label ---> 0\n",
    " * Second Label ---> 1\n",
    " * Third Label ---> 2\n",
    "\n",
    "## Ordinal Encoding:\n",
    " * Bad ---> 0\n",
    " * Average ---> 1\n",
    " * Good ---> 2\n",
    "\n",
    "## One-Hot Encoding\n",
    "* First Label ---> [1 0 0]\n",
    "* Second Label ---> [0 1 0]\n",
    "* Third Label ---> [0 0 1]\n",
    "\n",
    "Remark: one-hot encoding technically speaking converts our feature to three new features\n",
    "\n",
    "\n",
    "## Target Encoding\n",
    "A bit fancier. Encodes the probability of label being 1 given that category. May lead to overfitting, so use with care!\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Naive label encoding may not be always appropriate, since it introduces an order to our categories that may not have existed before.\n",
    "However, one-hot encoding can be costly to implement due to its tendency to introduce a high number of features.\n",
    "\n",
    "For this workshop, we will stick to label encoders for simplicity, and because we are going to train a tree model. Tree models tend to be less susceptible to artifacts introduced by label encoders."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_ENCODERS = {}\n",
    "TARGET_ENCODERS = {}\n",
    "SCALER = StandardScaler()\n",
    "\n",
    "log_cols = ['annual_inc', 'dti', 'int_rate', 'open_acc', 'total_acc']\n",
    "\n",
    "# every feature encoded need their label encoder\n",
    "# the scaler, however, can act on multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df, num_cols, cat_cols, log_cols=None, is_train=True):\n",
    "    ''' This function will be used to take the log of our features, scale our features, encodings, etc.'''\n",
    "\n",
    "    # taking the log of features in log_cols\n",
    "    # a problem is that at least one feature has neg values, so we are going to drop those rows\n",
    "    df = df[df['dti'] > -1]\n",
    "    if log_cols:\n",
    "        for col in log_cols:\n",
    "            df[col] = np.log1p(df[col])\n",
    "    \n",
    "\n",
    "    if is_train:\n",
    "        df[num_cols] = SCALER.fit_transform(df[num_cols])\n",
    "    else:\n",
    "        df[num_cols] = SCALER.transform(df[num_cols])\n",
    "\n",
    "\n",
    "    # some feature engineering specific to a variable\n",
    "    df['emp_length'] = df['emp_length'].apply(lambda x: '0' if x is '< 1 year' else x)\n",
    "    df['emp_length'] = df['emp_length'].str.replace('[^0-9]+', '')\n",
    "\n",
    "    cat_cols = list(cat_cols)\n",
    "    cat_cols.remove('emp_length')\n",
    "    cat_cols.remove('purpose')\n",
    "\n",
    "    # target encoding for high cardinality variable\n",
    "    if is_train:\n",
    "        TARGET_ENCODERS['purpose'] = TargetEncoder()\n",
    "        df['purpose'] = TARGET_ENCODERS['purpose'].fit_transform(X=df['purpose'], y=df['target'])\n",
    "    else:\n",
    "        df['purpose'] = TARGET_ENCODERS['purpose'].transform(X=df['purpose'])\n",
    "\n",
    "    for col in cat_cols:\n",
    "        encoder = LabelEncoder()\n",
    "        if is_train:\n",
    "            df[col] = encoder.fit_transform(df[col])\n",
    "            LABEL_ENCODERS[col] = encoder\n",
    "        else:\n",
    "            df[col] =LABEL_ENCODERS[col].transform(df[col])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = preprocessing(train_df, num_cols, cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df2 = feature_engineering(train_df, num_cols, cat_cols, log_cols=log_cols, is_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = preprocessing(test_df, num_cols, cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df2 = feature_engineering(test_df, num_cols, cat_cols, log_cols=log_cols, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['addr_state', 'annual_inc', 'application_type', 'dti',\n",
       "       'earliest_cr_line', 'emp_length', 'fico_range_high', 'fico_range_low',\n",
       "       'grade', 'home_ownership', 'initial_list_status', 'installment',\n",
       "       'int_rate', 'issue_d', 'loan_amnt', 'mort_acc', 'open_acc', 'pub_rec',\n",
       "       'pub_rec_bankruptcies', 'purpose', 'revol_bal', 'revol_util',\n",
       "       'sub_grade', 'term', 'title', 'total_acc', 'verification_status',\n",
       "       'target'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 76
    }
   ],
   "source": [
    "train_df2.columns"
   ]
  },
  {
   "source": [
    "We will now drop some columns that we do not intend to use for various reasons:\n",
    "\n",
    "- fico_range_high and fico_range_low are highly correlated, so let us keep one of them\n",
    "- title and purpose are related; let us keep purpose, which we target encoded\n",
    "- installment and loan_amnt are highly correlated, we keep loan_amnt,\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['addr_state', 'annual_inc', 'application_type', 'dti',\n",
    "       'emp_length', 'fico_range_high', 'grade', 'home_ownership', 'initial_list_status',\n",
    "       'loan_amnt', 'int_rate', 'mort_acc',\n",
    "       'open_acc', 'pub_rec', 'pub_rec_bankruptcies', 'purpose', 'revol_bal',\n",
    "       'revol_util', 'sub_grade', 'term', 'total_acc',\n",
    "       'verification_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df2[cols]\n",
    "y_train = train_df2['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_df2[cols]\n",
    "y_test = test_df2['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save datasets for modelling\n",
    "feather.write_dataframe(X_train, DATA_PATH / 'X_train')\n",
    "feather.write_dataframe(pd.DataFrame(y_train), DATA_PATH / 'y_train')\n",
    "feather.write_dataframe(X_test, DATA_PATH / 'X_test')\n",
    "feather.write_dataframe(pd.DataFrame(y_test), DATA_PATH / 'y_test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}